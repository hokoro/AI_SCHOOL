{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cheonWEEK7_sentiment_analysis_with_attention_class_2.ipynb","provenance":[{"file_id":"1hJViSP1JGLL4QgI3Go9yL5jkJUxddUnM","timestamp":1631670649818}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-DeDnUfhrHpC"},"source":["# Sentiment Analysis with Attention Mechanism\n","- author: Eu-Bin KIM\n","- date: 15th of September 2021\n"]},{"cell_type":"code","metadata":{"id":"QAKqjztinzvz","executionInfo":{"status":"ok","timestamp":1631670704309,"user_tz":-540,"elapsed":4529,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["\"\"\"\n","목표  = 감성분석기, LSTM.\n","\"\"\"\n","from typing import List, Tuple\n","import numpy as np\n","from keras_preprocessing.sequence import pad_sequences\n","from keras_preprocessing.text import Tokenizer\n","from torch.nn import functional as F\n","import torch"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yT2R0hzYowcs"},"source":["## TODO - 1 어텐션 마스크 만들기\n","- 어텐션 마스크란?\n","- 왜 마스크를 만들어야 할까?\n","- `torch.where` "]},{"cell_type":"code","metadata":{"id":"M0p_Qgc_o4Aq","executionInfo":{"status":"ok","timestamp":1631670704310,"user_tz":-540,"elapsed":10,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["DATA: List[Tuple[str, int]] = [\n","    # 긍정적인 문장 - 1\n","    (\"나는 자연어처리가 좋아.\", 1),\n","    (\"나는 자연어처리.\", 1),\n","    (\"도움이 되었으면.\", 1),\n","    # 병국님\n","    (\"오늘도 수고했어.\", 1),\n","    # 영성님\n","    (\"너는 할 수 있어.\", 1),\n","    # 정무님\n","    (\"오늘 내 주식이 올랐다.\", 1),\n","    # 우철님\n","    (\"오늘 날씨가 좋다.\", 1),\n","    # 유빈님\n","    (\"난 너를 좋아해.\", 1),\n","    # 다운님\n","    (\"지금 정말 잘하고 있어.\", 1),\n","    # 민종님\n","    (\"지금처럼만 하면 잘될거야.\", 1),\n","    (\"사랑해.\", 1),\n","    (\"저희 허락없이 아프지 마세요.\", 1),\n","    (\"오늘 점심 맛있다.\", 1),\n","    (\"오늘 너무 예쁘다.\", 1),\n","    # 다운님\n","    (\"곧 주말이야.\", 1),\n","    # 재용님\n","    (\"오늘 주식이 올랐어.\", 1),\n","    # 병운님\n","    (\"우리에게 빛나는 미래가 있어.\", 1),\n","    # 재용님\n","    (\"너는 참 잘생겼어.\", 1),\n","    # 윤서님\n","    (\"콩나물 무침은 맛있어.\", 1),\n","    # 정원님\n","    (\"강사님 보고 싶어요.\", 1),\n","    # 정원님\n","    (\"오늘 참 멋있었어.\", 1),\n","    # 예은님\n","    (\"맛있는게 먹고싶다.\", 1),\n","    # 민성님\n","    (\"로또 당첨됐어.\", 1),\n","    # 민성님\n","    (\"이 음식은 맛이 없을수가 없어.\", 1),\n","    # 경서님\n","    (\"오늘도 좋은 하루보내요.\", 1),\n","    # 성민님\n","    (\"내일 시험 안 본대.\", 1),\n","    # --- 부정적인 문장 - 레이블 = 0\n","    (\"난 너를 싫어해.\", 0),\n","    # 병국님\n","    (\"넌 잘하는게 뭐냐?.\", 0),\n","    # 선희님\n","    (\"너 때문에 다 망쳤어.\", 0),\n","    # 정무님\n","    (\"오늘 피곤하다.\", 0),\n","    # 유빈님\n","    (\"난 삼성을 싫어해.\", 0),\n","    (\"진짜 가지가지 한다.\", 0),\n","    (\"꺼져.\", 0),\n","    (\"그렇게 살아서 뭘하겠니.\", 0),\n","    # 재용님 - 주식이 파란불이다?\n","    (\"오늘 주식이 파란불이야.\", 0),\n","    # 지현님\n","    (\"나 오늘 예민해.\", 0),\n","    (\"주식이 떨어졌다.\", 0),\n","    (\"콩나물 다시는 안먹어.\", 0),\n","    (\"코인 시즌 끝났다.\", 0),\n","    (\"배고파 죽을 것 같아.\", 0),\n","    (\"한강 몇도냐.\", 0),\n","    (\"집가고 싶다.\", 0),\n","    (\"나 보기가 역겨워.\", 0),  # 긍정적인 확률이 0\n","    # 진환님\n","    (\"잘도 그러겠다.\", 0),\n","    (\"너는 어렵다.\", 0),\n","    # (\"자연어처리는\", 0)\n","]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"b28q0XW1o9zw","executionInfo":{"status":"ok","timestamp":1631670704310,"user_tz":-540,"elapsed":9,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["    # 문장을 다 가져온다\n","    sents = [sent for sent, label in DATA]\n","    # 레이블\n","    labels = [label for sent, label in DATA]\n","    # 정수인코딩\n","    tokenizer = Tokenizer(char_level=True, filters=\" \")\n","    tokenizer.fit_on_texts(texts=sents)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KEUOGueGzAd8","executionInfo":{"status":"ok","timestamp":1631670704310,"user_tz":-540,"elapsed":8,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["# builders # - 기존의 build_X로는 부족하다\n","def build_X(sents: List[str], tokenizer: Tokenizer, max_length: int) -> Tuple[torch.Tensor, torch.Tensor]:\n","    seqs = tokenizer.texts_to_sequences(texts=sents)\n","    seqs = pad_sequences(sequences=seqs, padding=\"post\", maxlen=max_length, value=0)\n","    X = torch.LongTensor(seqs)  # (N, L)\n","    return X"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pzlp6EbhzA_2","executionInfo":{"status":"ok","timestamp":1631670704311,"user_tz":-540,"elapsed":9,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}},"outputId":"37ca9326-e537-4edc-9896-d72fc88fb495"},"source":["build_X(sents, tokenizer, max_length=30)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[10,  8,  1,  ...,  0,  0,  0],\n","        [10,  8,  1,  ...,  0,  0,  0],\n","        [16, 63,  7,  ...,  0,  0,  0],\n","        ...,\n","        [10,  1, 34,  ...,  0,  0,  0],\n","        [21, 16,  1,  ...,  0,  0,  0],\n","        [11,  8,  1,  ...,  0,  0,  0]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ZxiYk7_0Jiy","executionInfo":{"status":"ok","timestamp":1631670704991,"user_tz":-540,"elapsed":686,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}},"outputId":"239b632b-03b7-45af-e072-27fddcb22e72"},"source":["# torch.where\n","x = torch.randn(3, 2)  # 랜덤한 값으로 채워진 벡터\n","y = torch.ones(3, 2)  # 1로 채워진 벡터\n","print(x)\n","print(y)\n","# x > 0을 만족하는 성분 -> x.\n","# x > 0을 만족하지않는 성분 -> y로 대체.\n","torch.where(x > 0, x, y)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.4717, -0.9762],\n","        [ 0.1052,  0.0489],\n","        [-0.1339, -0.2968]])\n","tensor([[1., 1.],\n","        [1., 1.],\n","        [1., 1.]])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 1.0000],\n","        [0.1052, 0.0489],\n","        [1.0000, 1.0000]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Ukt4MoohoLBr","executionInfo":{"status":"ok","timestamp":1631670704991,"user_tz":-540,"elapsed":11,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["def build_X_M(sents: List[str], tokenizer: Tokenizer, max_length: int) -> Tuple[torch.Tensor, torch.Tensor]:\n","    seqs = tokenizer.texts_to_sequences(texts=sents)\n","    seqs = pad_sequences(sequences=seqs, padding=\"post\", maxlen=max_length, value=0)\n","    X = torch.LongTensor(seqs)  # (N, L)\n","    ### TODO 1 ###\n","    # 마스크 만들기\n","    # 병국님\n","    # 마스크. padding 인 경우 = 0, 나머지는 = 1   # (N, L)\n","    M = torch.where(X > 0, 1, 0)\n","    # 영성님\n","    # M = torch.where(X!= 0, 1, 0)\n","    ##############\n","    return X, M  # (N, L), (N, L)\n","\n","\n","def build_y(labels: List[int]) -> torch.Tensor:\n","    return torch.FloatTensor(labels)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USY6cPXT09hW","executionInfo":{"status":"ok","timestamp":1631670704991,"user_tz":-540,"elapsed":10,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}},"outputId":"a13a5ffd-f466-4569-9beb-c8449a751356"},"source":["# d# 영성님: LongTensor는 무슨 역할인가요?\n","E = torch.nn.Embedding(10, 64)\n","# 아래 코드는 오류가 뜬다. \n","# print(E(torch.Tensor([1, 2, 3])))\n","# embedding 네트워크가 인덱스의 인자로 LongTensor를 강요하기 때문.\n","print(E(torch.LongTensor([1, 2, 3])))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0146,  0.3686,  0.6671, -1.1528, -2.6711,  1.2755, -0.0865, -1.1563,\n","          0.1837,  0.5282,  1.9119, -0.5711, -0.7047,  0.2241,  0.2362,  1.2183,\n","          0.0177,  0.6957,  1.3953, -1.9309,  0.1631,  0.9904,  0.5861, -0.6604,\n","          0.9811,  1.1324, -0.8985, -0.8515, -1.2280,  1.2271, -0.4997,  0.3321,\n","          0.4637, -0.0713, -1.5733,  0.6771, -1.5003, -0.1242, -0.2625,  0.4545,\n","          0.7711, -1.2478, -1.2969,  0.9249,  2.1004, -0.3166, -0.6201, -0.6980,\n","          1.6128, -0.0226,  2.1733,  0.0689, -0.9721, -0.0523,  0.1792,  1.0981,\n","         -1.6370,  0.8989, -0.3226, -0.2524, -1.1136,  0.9384,  1.0883,  0.1730],\n","        [-0.1249,  0.2623,  2.6501, -0.1627, -1.4293,  0.1579, -1.3194, -1.3430,\n","          0.3644, -0.3772, -0.2126, -0.5282, -0.9891, -0.2734, -0.5706, -0.5312,\n","         -0.6117, -0.3881, -0.9872, -1.5894, -0.1677,  0.5070, -0.8442,  1.0625,\n","          0.9627, -0.5989,  0.6494, -0.7245,  0.1330,  1.3284,  0.5525,  0.5281,\n","          1.7450, -0.8535, -0.9440,  0.0915, -0.3030,  0.0767,  1.1431, -0.3010,\n","          1.3132,  0.8954,  1.9157,  0.4168, -0.9714,  0.3115,  2.2838, -1.3186,\n","          1.7826, -0.2346,  0.2425, -0.3707,  0.2579, -2.3377,  0.4025, -0.9503,\n","         -1.6512,  0.0205,  0.4375, -1.3731,  1.5246, -0.4020, -1.9692, -0.2032],\n","        [ 2.1557,  1.9649, -1.5154, -0.4022,  0.8532, -0.9225,  0.7550, -1.3820,\n","          0.2850, -0.7752,  1.2063, -1.1188, -1.3820, -1.1664,  0.3785, -0.8526,\n","          0.1188,  1.8433,  0.8022, -1.5541, -1.3943,  2.0419,  0.3498,  0.4025,\n","          1.3147, -0.5900,  0.8541,  0.3363, -0.1175, -0.2806,  1.2216, -0.9475,\n","          0.5843,  0.8950,  0.1928, -1.5379, -1.4997,  0.6772,  0.0459, -0.0313,\n","         -0.0970, -0.2680,  0.7103, -0.7492, -0.4114, -0.3361,  1.3203, -1.6662,\n","         -0.1728, -0.9517,  1.1843, -0.8639, -0.5579,  0.8395, -0.9827,  0.2792,\n","         -0.7327, -0.7763, -0.5324, -0.2149, -0.0037, -0.2561, -0.6176,  0.7994]],\n","       grad_fn=<EmbeddingBackward>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"VB84VwS4qKc5"},"source":["## TODO 2 - `SimpleLSTMWithAttention`의 `forward` 함수 재정의하기\n","\n","H_last 만을 출력하는게 아니라, H_all 도 출력하는 함수로 변경하기.\n"]},{"cell_type":"code","metadata":{"id":"oCBl4iLe4bvd","colab":{"base_uri":"https://localhost:8080/","height":186},"executionInfo":{"status":"error","timestamp":1631670704995,"user_tz":-540,"elapsed":12,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}},"outputId":"de533dac-c5da-4eee-bbdb-dfa362ae1f15"},"source":["#  이렇게 시작해보세요 ! 미리 바구니 만들어보기.\n","H_all = torch.zeros(size=(...,...,...))  # (N, L, H)"],"execution_count":9,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-102eb7a74953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  이렇게 시작해보세요 ! 미리 바구니 만들어보기.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mH_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (N, L, H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: zeros(): argument 'size' must be tuple of ints, not tuple"]}]},{"cell_type":"code","metadata":{"id":"Hp5VcJhzoRJ0","executionInfo":{"status":"aborted","timestamp":1631670704992,"user_tz":-540,"elapsed":7,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["class SimpleLSTMWithAttention(torch.nn.Module):\n","    def __init__(self, vocab_size: int, hidden_size: int, embed_size: int):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        self.E = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n","        # 학습해야하는 가중치.\n","        # 학습해야하는 신경망을 다 정의해보자.\n","        # 1. 정의를 하는 것 스킵\n","        # 2. forward에서 데이터의 흐름을 *차원의 변화를 트래킹*하면서 확인\n","        # 3. (A, B) * (B, C) -> (A, C)\n","        self.W = torch.nn.Linear(in_features=hidden_size + embed_size, out_features=hidden_size*4)\n","        self.W_hy = torch.nn.Linear(in_features=hidden_size*2, out_features=1)\n","\n","    def training_step(self, X: torch.Tensor, M: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","        y_pred, _ = self.predict(X, M)  # (N, L) -> (N, 1)\n","        y_pred = torch.reshape(y_pred, y.shape)  # y와 차원의 크기를 동기화\n","        loss = F.binary_cross_entropy(y_pred, y)  # 이진분류 로스\n","        loss = loss.sum()  # 배치 속 모든 데이터 샘플에 대한 로스를 하나로\n","        return loss\n","\n","    def forward(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        h_t = f_W(h_t-1, xt)\n","        :param X: (N, L) = (배치의 크기, 문장의 길이)\n","        :return: (H_all, H_last) (N, L, H), (N, H)\n","        \"\"\"\n","        # 이제 뭐해요?\n","        # 반복문\n","        # 0 -> ???=L-1\n","        C_t = torch.zeros(size=(X.shape[0], self.hidden_size))  # (?=N, ?=H)\n","        H_t = torch.zeros(size=(X.shape[0], self.hidden_size))  # (?=N, ?=H)\n","        H_all = torch.zeros(size=(X.shape[0],X.shape[1],self.hidden_size))   # (N, L, H)\n","        ### TODO 2 ###  \n","        # H_all (문장에 대한 모든 단기기억 만들기.)\n","        for time in range(X.shape[1]):\n","            # 여기선 뭐하죠?\n","            X_t = X[:, time]  # (N, L) -> (N, 1)\n","            X_t = self.E(X_t)  # (N, 1) -> (N, E)\n","            # 이제 뭐하죠?\n","            H_cat_X = torch.cat([H_t, X_t], dim=1)  # (N, H), (N, E) -> (?=N,?=H+E)\n","            G = self.W(H_cat_X)  # (N, H+E) * (H+E, H*4) -> (N, H*4)\n","            F = G[:, :self.hidden_size]  # 큰 박스 안에 작은 박스를 넣는다.\n","            I = G[:, self.hidden_size:self.hidden_size*2]  # 큰 박스 안에 작은 박스를 넣는다.\n","            O = G[:, self.hidden_size*2:self.hidden_size*3]  # 큰 박스 안에 작은 박스를 넣는다.\n","            H_temp = G[:, self.hidden_size*3:self.hidden_size*4]  # 큰 박스 안에 작은 박스를 넣는다.\n","            # 이제 뭐하죠? - 활성화 함수,\n","            F = torch.sigmoid(F)  #  (N ,H) -> (N, H)\n","            I = torch.sigmoid(I)  #  (N ,H) -> (N, H)\n","            O = torch.sigmoid(O)  #  (N ,H) -> (N, H)\n","            H_temp = torch.tanh(H_temp)  #  (N ,H) -> (N, H)\n","            # 이제 뭐하죠?\n","            # 1. F의 용도? = 지우개\n","            # 2. I의 용도? = 필요한 기억 저장\n","            # 3. O의 용도? = 장기기억으로 부터 최종 단기기억 생성\n","            C_t = torch.mul(F, C_t) + torch.mul(I, H_temp)\n","            H_t = torch.mul(O, torch.tanh(C_t))\n","            H_all[:, time] = H_t  # 모든 N개의 데이터, 이 시간대 = H_t\n","        ##############\n","        H_last = H_t\n","        return H_all, H_last  # (N, L, H), (N, H).\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cIVDUXAnqlKF"},"source":["## TODO 3 - `SimpleLSTMWithAttention`의 `predict`함수 재정의 하기"]},{"cell_type":"markdown","metadata":{"id":"sMDuXlPPr_XO"},"source":["- `torch.einsum` 적극 활용하기!\n","  - 공식문서: https://pytorch.org/docs/stable/generated/torch.einsum.html\n","  - Einsum is all you need: https://rockt.github.io/2018/04/30/einsum\n","  - 간략한 정리(한글): https://ita9naiwa.github.io/numeric%20calculation/2018/11/10/Einsum.html \n"]},{"cell_type":"code","metadata":{"id":"QDa2Gzph_mhh","executionInfo":{"status":"aborted","timestamp":1631670704992,"user_tz":-540,"elapsed":7,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["# einsum = Domain Specific Language\n","a = torch.Tensor([1, 2, 3])  #(i, )\n","b = torch.Tensor([4, 5, 6])  #(i, )\n","# 이둘을 성분곱을 하고 싶다?\n","print(torch.mul(a, b))  # 파이토치를 알아야 한다.\n","print(torch.einsum(\"i,i->i\", a, b))  # 파이토치를 알필요가 없다. (성분곱)\n","print(torch.einsum(\"i,i->\", a, b))  # 파이토치를 알필요가 없다.  (내적) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qJQVktqJAl52","executionInfo":{"status":"aborted","timestamp":1631670704993,"user_tz":-540,"elapsed":8,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["# 그렇다면 행렬의 연산은?\n","A = torch.randn(size=(3, 10))\n","B = torch.randn(size=(10, 3))\n","print(A @ B)  # 행렬 곱\n","print(torch.einsum(\"xy,yz->xz\", A, B))\n","# a,b, c, d, ..z  어떤 캐릭터든 가능. "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1C5C2wnOqT5s","executionInfo":{"status":"aborted","timestamp":1631670704993,"user_tz":-540,"elapsed":8,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["def predict(self, X: torch.Tensor, M: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","  \"\"\"\n","  :param X: (N, L) inputs.\n","  :param M: (N, L) masks.\n","  \"\"\"\n","  H_all, H_last = self.forward(X)  # (N, L) -> (N, L, H),  (N, H).\n","  ### TODO 3 - 1 ###\n","  # 영성님\n","  S = torch.einsum(\"nlh,nh->nl\", H_all, H_last)\n","  # 병국님\n","  S = torch.einsum(\"nlh,hn->nl\", H_all, H_last.T)  # (N, L, H), (N, H) -> (N, L)\n","  # 어떻게 이해?\n","  # ein\"sum\" \n","  # sigma_h(A_nlh * B_nh) = -> nl.\n","  # mask the scores - we must set this to be negative.\n","  S[M == 0] = float(\"-inf\")\n","  A = torch.softmax(S, dim=1)  # attention scores.\n","  # compute weighted average to get the context.\n","  # 병국님:37\n","  # C = torch.einsum(\"NLH, NL -> NH\", H_all, S(x),A(o))\n","  # 천영성10:37\n","  # C = torch.einsum('nlh,nl -> nh',H_all,H_last(x), A(o))\n","  C = torch.einsum(\"nlh,nl->nh\", H_all, A) # (N, L, H), (N, L) -> (N, H). (H_all의 가중평균)\n","  # 정무님.\n","  # 다만 einsum은 concatenate를 할 때 사용하는 것은 아님\n","  C_cat_H = torch.cat([C, H_last], dim=1)  # (N, H), (N, H) -> (N, H*2)  (최종 feature)\n","  ##############\n","  y_pred = self.W_hy(C_cat_H)  # (N, H*2) * (H*2, 1) -> (N, 1)\n","  y_pred = torch.sigmoid(y_pred)  # (N, H) -> (N, 1)\n","  # print(y_pred)\n","  return y_pred, A\n","\n","# 함수 등록\n","SimpleLSTMWithAttention.predict = predict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oU7ZerqirBW-"},"source":["## Attention Score를 확인해보기!"]},{"cell_type":"code","metadata":{"id":"6HBvCi6ponLr","executionInfo":{"status":"aborted","timestamp":1631670704994,"user_tz":-540,"elapsed":9,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["class Analyser:\n","    \"\"\"\n","    lstm기반 감성분석기.\n","    \"\"\"\n","    def __init__(self, lstm: SimpleLSTMWithAttention, tokenizer: Tokenizer, max_length: int):\n","        self.lstm = lstm\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __call__(self, text: str) -> float:\n","        X, M = build_X_M(sents=[text], tokenizer=self.tokenizer, max_length=self.max_length)\n","        y_pred, A = self.lstm.predict(X, M)\n","        attentions = A.squeeze().detach().numpy()\n","        tokens = [\n","            \"[PAD]\" if idx == 0\n","            else self.tokenizer.index_word[idx]\n","            for idx in X.detach().squeeze().tolist()\n","        ]\n","        data = [\n","            (token, \"{:.2f}\".format(att))\n","            for token, att in zip(tokens, attentions)\n","        ]\n","        print(data)\n","        return y_pred.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1rgJeKkoopa","executionInfo":{"status":"aborted","timestamp":1631670704994,"user_tz":-540,"elapsed":9,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["# train & test\n","# --- hyper parameters --- #\n","EPOCHS = 800  # 에폭 \n","LR = 0.001  # 학습률\n","HIDDEN_SIZE = 32 \n","EMBED_SIZE = 12\n","MAX_LENGTH = 30  # 문장의 최대길이.\n","\n","# 데이터 셋을 구축. (X=(N, L, 2), y=(N, 1))\n","X, M = build_X_M(sents, tokenizer, MAX_LENGTH)\n","y = build_y(labels)\n","vocab_size = len(tokenizer.word_index.keys())\n","vocab_size += 1\n","# lstm with attention으로 감성분석 문제를 풀기.\n","lstm = SimpleLSTMWithAttention(vocab_size=vocab_size,hidden_size=HIDDEN_SIZE, embed_size=EMBED_SIZE)\n","optimizer = torch.optim.Adam(params=lstm.parameters(), lr=LR)\n","\n","for epoch in range(EPOCHS):\n","    loss = lstm.training_step(X, M, y)\n","    loss.backward()  # 오차 역전파\n","    optimizer.step()  # 경사도 하강\n","    optimizer.zero_grad()  #  다음 에폭에서 기울기가 축적이 되지 않도록 리셋\n","    print(epoch, \"-->\", loss.item())\n","\n","analyser = Analyser(lstm, tokenizer, MAX_LENGTH)\n","print(\"##### TRAIN TEST #####\")\n","for sent, label in DATA:\n","    print(sent, \"->\", label, analyser(sent))\n","print(\"##### TEST #####\")\n","sent = \"나는 자연어처리가 좋아\"\n","print(sent, \"->\", analyser(sent))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKRmiAGAHyTV","executionInfo":{"status":"aborted","timestamp":1631670704994,"user_tz":-540,"elapsed":9,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":["# 모델이 긍/부정을 판단할 때, 어떤 글자에 집중을 하는지 확인해보자! 우리의 직관과 일치하는가?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTy439-TJFvE","executionInfo":{"status":"aborted","timestamp":1631670704995,"user_tz":-540,"elapsed":10,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}}},"source":[""],"execution_count":null,"outputs":[]}]}