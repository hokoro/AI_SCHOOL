{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO09avHYO/awmOXMuiiXycI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wCcBd5myJwVQ"},"source":["나열 -> 타깃 나열 \n","\n","나열  -> 인코더  -> 디코더  -> 타깃 나열 \n","\n","나열 -> 인코더 블럭  -> 디코더 블럭 -> 타깃 나열"]},{"cell_type":"code","metadata":{"id":"uUwduP3cJtZB","colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"status":"error","timestamp":1632404855223,"user_tz":-540,"elapsed":405,"user":{"displayName":"천영성","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06905110897473075189"}},"outputId":"f89b67c1-2754-4d0d-8f8c-77d4611f80b9"},"source":["import torch\n","from typing import List, Tuple\n","from keras_preprocessing.sequence import pad_sequences\n","from keras_preprocessing.text import Tokenizer\n","\n","\n","class SelfAttention(torch.nn.Module):\n","    def __init__(self, embed_size: int, hidden_size: int):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.W_q = torch.nn.Linear(embed_size, hidden_size)\n","        self.W_k = torch.nn.Linear(embed_size, hidden_size)\n","        self.W_v = torch.nn.Linear(embed_size, hidden_size)\n","\n","    def forward(self, X_embed: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param X_embed: (N, L, E)\n","        :return: H_attn (N, L, H)\n","        \"\"\"\n","        # 셀프 어텐션에서는 각 세개의 신경망에 들어가는 입력이 동일하다.\n","        Q = self.W_q(X_embed)  # (N, L, E) * (?=E,?=H) -> (N, L, H)\n","        K = self.W_k(X_embed)  # (N, L, E) * (?=E,?=H) -> (N, L, H)\n","        V = self.W_v(X_embed)  # (N, L, E) * (?=E,?=H) -> (N, L, H)\n","        # 이 다음엔 뭘하죠?\n","        # Q와 K의 내적.\n","        Out = torch.einsum(\"nah,nbh->nab\", Q, K)  # 단어와 단어사이의 관계. -> (N, L, L)\n","        Out = Out / torch.sqrt(Out.shape[2])  # 매우 크거나 매우 작은수가 나올 확률을 줄여주는 것.\n","        Out = torch.softmax(Out, dim=1)  # [0, 1]사이로  정규화\n","        H_attn = torch.einsum(\"nll,nlh->nlh\", Out, V)  # Out(N, L, L), V(N, L, H)  -> (N, L, H)\n","        return H_attn\n","\n","\n","class EncoderBlock(torch.nn.Module):\n","    def __init__(self, embed_size: int):\n","        super().__init__()\n","        self.embed_size = embed_size\n","        # 학습해야하는 가중치?\n","        # 영성님 - self_attention\n","        # position-wise FFN\n","        self.sa = SelfAttention(embed_size=embed_size, hidden_size=embed_size)\n","        self.ffn = torch.nn.Linear(in_features=embed_size, out_features=embed_size)\n","\n","    def forward(self, X_embed: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param X_embed: (N, L, E)\n","        :return: (N, L, E)\n","        \"\"\"\n","        Out = self.sa(X_embed)  # (N, L, E) -> (N, L, E)\n","        # L, E * E, E -> L, E.\n","        Out = self.ffn(Out)  # (N, L, E) * (?=E,?=E) -> (N, L, E)\n","        return Out\n","\n","\n","class Transformer(torch.nn.Module):\n","    # 1. 신경망 속에 학습해야하는 가중치를 정의하는 메소드.\n","    # 2. 신경망의 출력을 계산하는 메소드.\n","    # 3. 신경망의 로스를 계산하는 메소드.\n","    def __init__(self, vocab_size: int, embed_size: int, depth: int):\n","        super().__init__()\n","        self.vocab_size = vocab_size\n","\n","        # --- 가중치를 정의하는 곳 --- #\n","        # 임베딩 테이블 정의\n","        self.token_embeddings = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n","        # 인코더 레이어 정의\n","        self.blocks = torch.nn.Sequential(\n","            *[EncoderBlock(embed_size) for _ in range(depth)]\n","        )\n","\n","    def forward(self, X: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param X: (N, L)\n","        :return:\n","        \"\"\"\n","        # 정수인코딩 -> 임베딩.\n","        X_embed = self.token_embeddings(X)  # (N, L) -> (N, L, E)\n","        # torch.nn.Sequential로 정의를 했기 때문에, blocks를 레이어 취급할 수 있다.\n","        H_all = self.blocks(X_embed)  # (N, L, E) -> (N, L, E)\n","        pass\n","\n","    def training_step(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","        loss: torch.Tensor = ...\n","        return loss\n","\n","\n","EPOCHS = 50\n","EMBED_SIZE = 512\n","MAX_LENGTH = 30\n","NUM_HEADS = 10\n","DEPTH = 3\n","LR = 0.0001\n","\n","DATA: List[Tuple[str, int]] = [\n","    # 긍정적인 문장 - 1\n","    (\"도움이 되었으면\", 1),\n","    # 병국님\n","    (\"오늘도 수고했어\", 1),\n","    # 영성님\n","    (\"너는 할 수 있어\", 1),\n","    # 정무님\n","    (\"오늘 내 주식이 올랐다\", 1),\n","    # 우철님\n","    (\"오늘 날씨가 좋다\", 1),\n","    # 유빈님\n","    (\"난 너를 좋아해\", 1),\n","    # 다운님\n","    (\"지금 정말 잘하고 있어\", 1),\n","    # 민종님\n","    (\"지금처럼만 하면 잘될거야\", 1),\n","    (\"사랑해\", 1),\n","    (\"저희 허락없이 아프지 마세요\", 1),\n","    (\"오늘 점심 맛있다\", 1),\n","    (\"오늘 너무 예쁘다\", 1),\n","    # 다운님\n","    (\"곧 주말이야\", 1),\n","    # 재용님\n","    (\"오늘 주식이 올랐어\", 1),\n","    # 병운님\n","    (\"우리에게 빛나는 미래가 있어\", 1),\n","    # 재용님\n","    (\"너는 참 잘생겼어\", 1),\n","    # 윤서님\n","    (\"콩나물 무침은 맛있어\", 1),\n","    # 정원님\n","    (\"강사님 보고 싶어요\", 1),\n","    # 정원님\n","    (\"오늘 참 멋있었어\", 1),\n","    # 예은님\n","    (\"맛있는게 먹고싶다\", 1),\n","    # 민성님\n","    (\"로또 당첨됐어\", 1),\n","    # 민성님\n","    (\"이 음식은 맛이 없을수가 없어\", 1),\n","    # 경서님\n","    (\"오늘도 좋은 하루보내요\", 1),\n","    # 성민님\n","    (\"내일 시험 안 본대\", 1),\n","    # --- 부정적인 문장 - 레이블 = 0\n","    (\"난 너를 싫어해\", 0),\n","    # 병국님\n","    (\"넌 잘하는게 뭐냐?\", 0),\n","    # 선희님\n","    (\"너 때문에 다 망쳤어\", 0),\n","    # 정무님\n","    (\"오늘 피곤하다\", 0),\n","    # 유빈님\n","    (\"난 삼성을 싫어해\", 0),\n","    (\"진짜 가지가지 한다\", 0),\n","    (\"꺼져\", 0),\n","    (\"그렇게 살아서 뭘하겠니\", 0),\n","    # 재용님 - 주식이 파란불이다?\n","    (\"오늘 주식이 파란불이야\", 0),\n","    # 지현님\n","    (\"나 오늘 예민해\", 0),\n","    (\"주식이 떨어졌다\", 0),\n","    (\"콩나물 다시는 안먹어\", 0),\n","    (\"코인 시즌 끝났다\", 0),\n","    (\"배고파 죽을 것 같아\", 0),\n","    (\"한강 몇도냐\", 0),\n","    (\"집가고 싶다\", 0),\n","    (\"나 보기가 역겨워\", 0),  # 긍정적인 확률이 0\n","    # 진환님\n","    (\"나는 너가 싫어\", 0),\n","    (\"잘도 그러겠다\", 0),\n","    (\"너는 어렵다\", 0),\n","    # (\"자연어처리는\", 0)\n","]\n","\n","TESTS = [\n","    \"나는 자연어처리가 좋아\",\n","    \"나는 자연어처리가 싫어\",\n","    \"나는 너가 좋다\",\n","    \"너는 참 좋다\",\n","]\n","\n","\n","# builders #\n","def build_X(sents: List[str], tokenizer: Tokenizer, max_length: int) -> torch.Tensor:\n","    seqs = tokenizer.texts_to_sequences(texts=sents)\n","    seqs = pad_sequences(sequences=seqs, padding=\"post\", maxlen=max_length, value=0)\n","    return torch.LongTensor(seqs)\n","\n","\n","def build_y(labels: List[int]) -> torch.Tensor:\n","    return torch.FloatTensor(labels)\n","\n","\n","class Analyser:\n","    \"\"\"\n","    lstm기반 감성분석기.\n","    \"\"\"\n","    def __init__(self, transformer: Transformer, tokenizer: Tokenizer, max_length: int):\n","        self.transformer = transformer\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __call__(self, text: str) -> float:\n","        X = build_X(sents=[text], tokenizer=self.tokenizer, max_length=self.max_length)\n","        y_pred = self.transformer.forward(X)\n","        return y_pred.item()\n","\n","\n","def main():\n","    # 문장을 다 가져온다\n","    sents = [sent for sent, label in DATA]\n","    # 레이블\n","    labels = [label for sent, label in DATA]\n","    # 정수인코딩\n","    tokenizer = Tokenizer(char_level=True)\n","    tokenizer.fit_on_texts(texts=sents)\n","    # 데이터 셋을 구축. (X=(N, L), y=(N, 1))\n","    X = build_X(sents, tokenizer, MAX_LENGTH)\n","    y = build_y(labels)\n","\n","    vocab_size = len(tokenizer.word_index.keys())\n","    vocab_size += 1\n","    # lstm 으로 감성분석 문제를 풀기.\n","    lstm = Transformer(embed_size=EMBED_SIZE, vocab_size=vocab_size, num_heads=NUM_HEADS, depth=DEPTH, max_length=MAX_LENGTH)\n","    optimizer = torch.optim.Adam(params=lstm.parameters(), lr=LR)\n","    for epoch in range(EPOCHS):\n","        loss = lstm.training_step(X, y)\n","        loss.backward()  # 오차 역전파\n","        optimizer.step()  # 경사도 하강\n","        optimizer.zero_grad()  # 다음 에폭에서 기울기가 축적이 되지 않도록 리셋\n","        print(epoch, \"-->\", loss.item())\n","\n","    analyser = Analyser(lstm, tokenizer, MAX_LENGTH)\n","    print(\"##### TEST #####\")\n","    for sent in TESTS:\n","        print(sent, \"->\", analyser(sent))\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":4,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-8e21d371826f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-8e21d371826f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mvocab_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# lstm 으로 감성분석 문제를 풀기.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_HEADS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEPTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'num_heads'"]}]},{"cell_type":"code","metadata":{"id":"fTQI6DciLlgc"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pqw73V6ALoIV"},"source":[""],"execution_count":null,"outputs":[]}]}