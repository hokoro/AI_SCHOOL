{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNsgoZjlSndZBZIRolXX5k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMdSg0ssDDeI","executionInfo":{"status":"ok","timestamp":1631199209323,"user_tz":-540,"elapsed":8961,"user":{"displayName":"천영성","photoUrl":"","userId":"06905110897473075189"}},"outputId":"27584be1-e831-4b45-dd4c-a141f72a357a"},"source":["  \n","\"\"\"\n","간단한 감성분석을 할 수 있는 간단한 RNN을 pytorch로 개발하는 것이 목표.\n","\"\"\"\n","\n","import torch\n","from typing import List, Tuple\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from keras_preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","\n","\n","# 신경망을 만들고 싶다 -> torch.nn.Module을 상속받기.\n","class SimpleRNN(torch.nn.Module):\n","    def __init__(self, vocab_size: int, hidden_size: int, embed_size: int):\n","        \"\"\"\n","        :param vocab_size: 말뭉치 속 고유한 단어의 개수. |V|\n","        :param hidden_size: rnn의 hidden vector의 차원의 크기. H\n","        \"\"\"\n","        # 학습하고자하는 가중치를 정의해주면 됩니다.\n","        super().__init__()\n","        # 정수인코딩된 나열을 그대로 입력으로 넣으면, 글자의 크기를 학습하게된다.\n","        # 그래서 원핫인코딩 or 임베딩.\n","        # Embedding layer = (|V|, E)\n","        self.hidden_size = hidden_size\n","        self.Embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n","        # h_t = f_W(h_t-1, x_t)\n","        # 정무님 - Wa, Wy, ba, by\n","        # 우리가 배웠던 기호 - W_hh, b_hh, Wxh, b_hh\n","        # h_t = tanh(W_hh * h_t-1 + W_xh * x_t)\n","        # h_t-1 (N, H) 과 h_t (N, H) 의 차원은 동일하다.\n","        # 즉. h_t-1 * W_hh  =  (N, H) * (H, H) = (N, H)\n","        # (N, L)\n","        # -> embeding layer -> (N, E) = x_t\n","        # x_t * W_xh = (N, E) * (E, H) =  (N, H)\n","        self.W_hh = torch.nn.Linear(in_features=hidden_size, out_features=hidden_size)  # (H, H)\n","        self.W_xh = torch.nn.Linear(in_features=embed_size, out_features=hidden_size)  # (E, H)\n","        self.W_hy = torch.nn.Linear(in_features=hidden_size, out_features=1)  # (H, 1)\n","\n","    def forward(self, X: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param X: 정수 인코딩된 나열의 배치 [[1, 2, 3, 0, 0], [5, 3, 2, 1, 0]]  (N, L)\n","        :return: 마지막 시간대의 hidden state를 출력 (N, H)\n","        \"\"\"\n","        # forward pass를 했을때 나오는 출력값을 계산\n","        # h_t = tanh(W_hh * h_t-1 + W_xh * x_t)\n","        # 첫번째 h_t는 무의미한 값이 들어간다.\n","        # H_0 = torch.zeros(size=(X.shape[0], self.hidden_size))\n","        # # 그 다음으로는 뭘 해야돼죠?\n","        # # 지금 H_0을 구했다.\n","        # # H_1 = tanh(H_0 * W_hh + X_1 * W_xh)\n","        # H_1 = torch.tanh(H_0 * self.W_hh + X[:, 0] * self.W_xh)\n","        # H_2 = torch.tanh(H_1 * self.W_hh + X[:, 1] * self.W_xh)\n","        # H_3 = torch.tanh(H_2 * self.W_hh + X[:, 2] * self.W_xh)\n","        # 그런데 이렇게 계속 반복을 하는 것보단, 일반화를 하는 것이 낫지 않을까?\n","        # # for loop\n","        H_t = torch.zeros(size=(X.shape[0], self.hidden_size))\n","        for t in range(X.shape[1]):\n","            Embed_t = self.Embedding(X[:, t])\n","            H_t = torch.tanh(self.W_hh(H_t) + self.W_xh(Embed_t))\n","        # many to one (sentiment analysis)\n","        return H_t\n","\n","    def training_step(self, X: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        :param X: 정수 인코딩된 나열의 배치 [[1, 2, 3, 0, 0], [5, 3, 2, 1, 0]]  (N, L)\n","        :param y: 긍/부정 레이블 [0, 1, 0, 0 , 1]\n","        :return: loss (1)\n","        \"\"\"\n","        # 배치가 들어왔을 때, 로스를 계산하면 된다.\n","        H_last = self.forward(X)  # (N, L) -> (N, H)\n","        # 이것으로부터, 긍정 / 부정일 확률을 구하고 싶다.\n","        # (N, H) * ???-(H, 1) -> (N, 1) - 어떻게 할 수 있을까?\n","        Hy = self.W_hy(H_last)  # (N, H) * (H, 1) -> (N, 1)\n","        # 그럼 Hy 의 범위가  [0, 1]?  아니다.\n","        # (N, 1)\n","        Hy_normalized = torch.sigmoid(Hy)  # [-inf, inf] -> [0, 1]\n","        # (N, 1), (N,) -> 하나로 통일하기 위해, y의 맞춘다.\n","        Hy_normalized = torch.reshape(Hy_normalized, y.shape)\n","        # N개의 로스를 다 더해서, 이 배치의 로스를 구한다.\n","        loss = F.binary_cross_entropy(input=Hy_normalized, target=y).sum()\n","        return loss\n","\n","# 어떤 데이터로 학습할까?\n","# 10개만 받겠습니다.\n","# 긍정적인 문장 5개를 적어주세요.\n","DATA: List[Tuple[str, int]] = [\n","    # 병국님\n","    (\"오늘도 수고했어\", 1),\n","    # 영성님\n","    (\"너는 할 수 있어\", 1),\n","    # 정무님\n","    (\"오늘 내 주식이 올랐다\", 1),\n","    # 우철님\n","    (\"오늘 날씨가 좋다\", 1),\n","    # 유빈님\n","    (\"난 너를 좋아해\", 1),\n","    # --- 부정적인 문장 - 레이블 = 0\n","    (\"난 너를 싫어해\", 0),\n","    # 병국님\n","    (\"넌 잘하는게 뭐냐?\", 0),\n","    # 선희님\n","    (\"너 때문에 다 망쳤어\", 0),\n","    # 정무님\n","    (\"오늘 피곤하다\", 0),\n","    # 유빈님\n","    (\"난 삼성을 싫어해\", 0)\n","]\n","\n","\n","# builders - 데이터를 텐서로 변환해주는 함수.\n","def build_X(sents: List[str], tokenizer: Tokenizer) -> torch.Tensor:\n","    tokenizer.fit_on_texts(sents)\n","    seqs = tokenizer.texts_to_sequences(sents)\n","    # 병국님, 영성님 - 패딩처리가 필요하다. 문장의 길이가 다 다르므로, 규격의 통일이 필요하다.\n","    seqs = pad_sequences(sequences=seqs, padding='post', value=0)\n","    return torch.LongTensor(seqs)\n","\n","\n","def build_y(labels: List[int]) -> torch.Tensor:\n","    return torch.FloatTensor(labels)\n","\n","\n","# 파이토치에서는 데이터셋을 정의해준느 것이 편하다.\n","class SimpleDataset(Dataset):\n","    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n","        self.X = X\n","        self.y = y\n","\n","    # 첫번째 - 데이터 셋의 크기를 알려주는 함수.\n","    def __len__(self) -> int:\n","        return self.y.shape[0]\n","\n","    def __getitem__(self, item: int) -> Tuple[torch.Tensor, torch.Tensor]:\n","        return self.X[item], self.y[item]\n","\n","\n","class SentimentAnalyzer:\n","    def __init__(self, rnn: SimpleRNN, tokenizer: Tokenizer):\n","        self.rnn = rnn\n","        self.tokenizer = tokenizer\n","\n","    def __call__(self, text: str) -> float:\n","        X = build_X(sents=[text], tokenizer=self.tokenizer)\n","        Ht = self.rnn.forward(X)\n","        Hy = self.rnn.W_hy(Ht)\n","        Hy_normalized = torch.sigmoid(Hy)\n","        return Hy_normalized.item()\n","\n","\n","def main():\n","    # 문장만,\n","    sents = [\n","        sent\n","        for sent, _ in DATA\n","    ]\n","    # 레이블만,\n","    labels = [\n","        label\n","        for _, label in DATA\n","    ]\n","    # 정무님: sents: List[str] -> List[int]\n","    tokenizer = Tokenizer(char_level=True)\n","    tokenizer.fit_on_texts(sents)\n","\n","    # 데이터 구축\n","    X = build_X(sents, tokenizer)\n","    y = build_y(labels)\n","    dataset = SimpleDataset(X, y)  # 파이토치 데이터 셋에 텐서를 저장.\n","    dataloader = DataLoader(dataset=dataset, batch_size=3, shuffle=True)\n","\n","    # 모델 만들기\n","    vocab_size = len(tokenizer.word_index) # 정수 1부터 부여.\n","    vocab_size += 1  # 왜 하나 더 늘려야할까? - padding token 0.\n","    rnn = SimpleRNN(vocab_size=vocab_size, hidden_size=16, embed_size=8)\n","\n","    # 어떤 최적화 알고리즘을 쓸까?\n","    optimizer = torch.optim.Adam(params=rnn.parameters(), lr=0.001)\n","\n","    # 에폭 루프\n","    for e_idx in range(300):\n","        # 배치루프\n","        losses = list()\n","        for b_idx, batch in enumerate(dataloader):\n","            X, y = batch\n","            # 영성님 - 학습을 해주어야 한다.\n","            # 그럼 무엇을 계산해야하는가\n","            loss: torch.Tensor = rnn.training_step(X, y)\n","            optimizer.zero_grad()\n","            loss.backward()  # 오차역전파 (모든 기울기를 다 계산)\n","            # 가중치를 경사도 하강법을 업데이트\n","            optimizer.step()\n","            # 기울기가 축적이 되는 것을 방지하기 위해 리셋\n","            losses.append(loss.item())\n","        avg_loss = (sum(losses) / len(losses))\n","        print(\"epoch:{}, avg_loss:{}\".format(e_idx, avg_loss))\n","\n","    analyser = SentimentAnalyzer(rnn, tokenizer)\n","    print(\"####-------- inferece ---- -#####\")\n","    print(analyser(text=\"오늘 날씨가 좋다\")) # 0.93의 확률로 출력!\n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch:0, avg_loss:0.7311403453350067\n","epoch:1, avg_loss:0.7159046679735184\n","epoch:2, avg_loss:0.7098070085048676\n","epoch:3, avg_loss:0.7212157994508743\n","epoch:4, avg_loss:0.6944862455129623\n","epoch:5, avg_loss:0.6969004720449448\n","epoch:6, avg_loss:0.6918914914131165\n","epoch:7, avg_loss:0.6808034926652908\n","epoch:8, avg_loss:0.7154216170310974\n","epoch:9, avg_loss:0.7125505954027176\n","epoch:10, avg_loss:0.6648824661970139\n","epoch:11, avg_loss:0.7048827856779099\n","epoch:12, avg_loss:0.675540417432785\n","epoch:13, avg_loss:0.6833036988973618\n","epoch:14, avg_loss:0.6795611381530762\n","epoch:15, avg_loss:0.6926344931125641\n","epoch:16, avg_loss:0.664396122097969\n","epoch:17, avg_loss:0.6847409307956696\n","epoch:18, avg_loss:0.6337667852640152\n","epoch:19, avg_loss:0.6602082252502441\n","epoch:20, avg_loss:0.6566364616155624\n","epoch:21, avg_loss:0.6791182905435562\n","epoch:22, avg_loss:0.6779254525899887\n","epoch:23, avg_loss:0.6530476808547974\n","epoch:24, avg_loss:0.6677213460206985\n","epoch:25, avg_loss:0.607061930000782\n","epoch:26, avg_loss:0.6419443190097809\n","epoch:27, avg_loss:0.5986773446202278\n","epoch:28, avg_loss:0.621348649263382\n","epoch:29, avg_loss:0.6612952053546906\n","epoch:30, avg_loss:0.6571736857295036\n","epoch:31, avg_loss:0.6469212472438812\n","epoch:32, avg_loss:0.6419145464897156\n","epoch:33, avg_loss:0.6349135488271713\n","epoch:34, avg_loss:0.5703379362821579\n","epoch:35, avg_loss:0.6332966536283493\n","epoch:36, avg_loss:0.6183402091264725\n","epoch:37, avg_loss:0.5871777683496475\n","epoch:38, avg_loss:0.554622121155262\n","epoch:39, avg_loss:0.6296103671193123\n","epoch:40, avg_loss:0.6147186309099197\n","epoch:41, avg_loss:0.6240697056055069\n","epoch:42, avg_loss:0.6128998175263405\n","epoch:43, avg_loss:0.5276102609932423\n","epoch:44, avg_loss:0.5868758708238602\n","epoch:45, avg_loss:0.5814004987478256\n","epoch:46, avg_loss:0.5792168974876404\n","epoch:47, avg_loss:0.5630205422639847\n","epoch:48, avg_loss:0.5221797153353691\n","epoch:49, avg_loss:0.5448797792196274\n","epoch:50, avg_loss:0.591869480907917\n","epoch:51, avg_loss:0.5624193325638771\n","epoch:52, avg_loss:0.5148740634322166\n","epoch:53, avg_loss:0.5412034168839455\n","epoch:54, avg_loss:0.5245902687311172\n","epoch:55, avg_loss:0.5053250193595886\n","epoch:56, avg_loss:0.4177434779703617\n","epoch:57, avg_loss:0.4073156416416168\n","epoch:58, avg_loss:0.44219282269477844\n","epoch:59, avg_loss:0.40008729696273804\n","epoch:60, avg_loss:0.3927733823657036\n","epoch:61, avg_loss:0.3807557262480259\n","epoch:62, avg_loss:0.35950450599193573\n","epoch:63, avg_loss:0.347484540194273\n","epoch:64, avg_loss:0.3177628554403782\n","epoch:65, avg_loss:0.3212147615849972\n","epoch:66, avg_loss:0.3157525658607483\n","epoch:67, avg_loss:0.3025936298072338\n","epoch:68, avg_loss:0.2662988640367985\n","epoch:69, avg_loss:0.25132961571216583\n","epoch:70, avg_loss:0.20969078689813614\n","epoch:71, avg_loss:0.2000609040260315\n","epoch:72, avg_loss:0.17104806751012802\n","epoch:73, avg_loss:0.15208235755562782\n","epoch:74, avg_loss:0.148570716381073\n","epoch:75, avg_loss:0.13839435018599033\n","epoch:76, avg_loss:0.13093111664056778\n","epoch:77, avg_loss:0.11406594887375832\n","epoch:78, avg_loss:0.11861755885183811\n","epoch:79, avg_loss:0.11504922993481159\n","epoch:80, avg_loss:0.10300957038998604\n","epoch:81, avg_loss:0.10027912817895412\n","epoch:82, avg_loss:0.09506413713097572\n","epoch:83, avg_loss:0.09093736112117767\n","epoch:84, avg_loss:0.08925004675984383\n","epoch:85, avg_loss:0.08371894061565399\n","epoch:86, avg_loss:0.07990071550011635\n","epoch:87, avg_loss:0.07796802744269371\n","epoch:88, avg_loss:0.07588806189596653\n","epoch:89, avg_loss:0.07282607443630695\n","epoch:90, avg_loss:0.0710007194429636\n","epoch:91, avg_loss:0.06863579247146845\n","epoch:92, avg_loss:0.06128881499171257\n","epoch:93, avg_loss:0.06378659885376692\n","epoch:94, avg_loss:0.06178079638630152\n","epoch:95, avg_loss:0.05904768314212561\n","epoch:96, avg_loss:0.05732210539281368\n","epoch:97, avg_loss:0.05590841732919216\n","epoch:98, avg_loss:0.054070403799414635\n","epoch:99, avg_loss:0.049879866652190685\n","epoch:100, avg_loss:0.05325605720281601\n","epoch:101, avg_loss:0.050973924808204174\n","epoch:102, avg_loss:0.04964105039834976\n","epoch:103, avg_loss:0.048482430167496204\n","epoch:104, avg_loss:0.04637025110423565\n","epoch:105, avg_loss:0.04505341313779354\n","epoch:106, avg_loss:0.044485338032245636\n","epoch:107, avg_loss:0.04294873680919409\n","epoch:108, avg_loss:0.04258068650960922\n","epoch:109, avg_loss:0.041912092827260494\n","epoch:110, avg_loss:0.040665808133780956\n","epoch:111, avg_loss:0.039791204035282135\n","epoch:112, avg_loss:0.039300727657973766\n","epoch:113, avg_loss:0.03818586003035307\n","epoch:114, avg_loss:0.03696277644485235\n","epoch:115, avg_loss:0.03712736163288355\n","epoch:116, avg_loss:0.0364020736888051\n","epoch:117, avg_loss:0.03478899132460356\n","epoch:118, avg_loss:0.033997707068920135\n","epoch:119, avg_loss:0.03378662373870611\n","epoch:120, avg_loss:0.03333317441865802\n","epoch:121, avg_loss:0.03253412153571844\n","epoch:122, avg_loss:0.0326938652433455\n","epoch:123, avg_loss:0.031983822118490934\n","epoch:124, avg_loss:0.03153470251709223\n","epoch:125, avg_loss:0.030242031905800104\n","epoch:126, avg_loss:0.02943761833012104\n","epoch:127, avg_loss:0.029689726419746876\n","epoch:128, avg_loss:0.028380994219332933\n","epoch:129, avg_loss:0.02822081372141838\n","epoch:130, avg_loss:0.027440561447292566\n","epoch:131, avg_loss:0.02730645053088665\n","epoch:132, avg_loss:0.02746142214164138\n","epoch:133, avg_loss:0.026976823806762695\n","epoch:134, avg_loss:0.026627905201166868\n","epoch:135, avg_loss:0.026252537965774536\n","epoch:136, avg_loss:0.025854486040771008\n","epoch:137, avg_loss:0.024843917228281498\n","epoch:138, avg_loss:0.02422797167673707\n","epoch:139, avg_loss:0.024721655994653702\n","epoch:140, avg_loss:0.02411404438316822\n","epoch:141, avg_loss:0.02408409398049116\n","epoch:142, avg_loss:0.023563184775412083\n","epoch:143, avg_loss:0.022476832382380962\n","epoch:144, avg_loss:0.022191431373357773\n","epoch:145, avg_loss:0.022416782099753618\n","epoch:146, avg_loss:0.022286605089902878\n","epoch:147, avg_loss:0.021782847121357918\n","epoch:148, avg_loss:0.02179358247667551\n","epoch:149, avg_loss:0.021410015877336264\n","epoch:150, avg_loss:0.020889695268124342\n","epoch:151, avg_loss:0.020941203460097313\n","epoch:152, avg_loss:0.02042922144755721\n","epoch:153, avg_loss:0.020401393063366413\n","epoch:154, avg_loss:0.02013937523588538\n","epoch:155, avg_loss:0.01986286509782076\n","epoch:156, avg_loss:0.018905187025666237\n","epoch:157, avg_loss:0.01866696961224079\n","epoch:158, avg_loss:0.019064880907535553\n","epoch:159, avg_loss:0.018891325686126947\n","epoch:160, avg_loss:0.018476540222764015\n","epoch:161, avg_loss:0.0179323204793036\n","epoch:162, avg_loss:0.01753652561455965\n","epoch:163, avg_loss:0.018066035583615303\n","epoch:164, avg_loss:0.017616848926991224\n","epoch:165, avg_loss:0.01629258901812136\n","epoch:166, avg_loss:0.017382832476869226\n","epoch:167, avg_loss:0.017022649757564068\n","epoch:168, avg_loss:0.01651006145402789\n","epoch:169, avg_loss:0.016821084078401327\n","epoch:170, avg_loss:0.016452587209641933\n","epoch:171, avg_loss:0.01581146032549441\n","epoch:172, avg_loss:0.0160923742223531\n","epoch:173, avg_loss:0.01489606173709035\n","epoch:174, avg_loss:0.015436975285410881\n","epoch:175, avg_loss:0.015145233599469066\n","epoch:176, avg_loss:0.015568353934213519\n","epoch:177, avg_loss:0.01540343463420868\n","epoch:178, avg_loss:0.014668670948594809\n","epoch:179, avg_loss:0.014940005959942937\n","epoch:180, avg_loss:0.01473283558152616\n","epoch:181, avg_loss:0.014203289290890098\n","epoch:182, avg_loss:0.014580956194549799\n","epoch:183, avg_loss:0.014335526619106531\n","epoch:184, avg_loss:0.014109023613855243\n","epoch:185, avg_loss:0.013957184739410877\n","epoch:186, avg_loss:0.01349496841430664\n","epoch:187, avg_loss:0.013653410831466317\n","epoch:188, avg_loss:0.013228080933913589\n","epoch:189, avg_loss:0.013599051395431161\n","epoch:190, avg_loss:0.013537823455408216\n","epoch:191, avg_loss:0.013240809785202146\n","epoch:192, avg_loss:0.012260248884558678\n","epoch:193, avg_loss:0.012589685618877411\n","epoch:194, avg_loss:0.012579181464388967\n","epoch:195, avg_loss:0.012743734987452626\n","epoch:196, avg_loss:0.012236656388267875\n","epoch:197, avg_loss:0.012505569960922003\n","epoch:198, avg_loss:0.011583956656977534\n","epoch:199, avg_loss:0.012426301604136825\n","epoch:200, avg_loss:0.012313821818679571\n","epoch:201, avg_loss:0.012175550684332848\n","epoch:202, avg_loss:0.011165784439072013\n","epoch:203, avg_loss:0.011064049787819386\n","epoch:204, avg_loss:0.011364700505509973\n","epoch:205, avg_loss:0.011263265972957015\n","epoch:206, avg_loss:0.011416437802836299\n","epoch:207, avg_loss:0.011533061973750591\n","epoch:208, avg_loss:0.011438298504799604\n","epoch:209, avg_loss:0.010956745827570558\n","epoch:210, avg_loss:0.011127607431262732\n","epoch:211, avg_loss:0.010914488695561886\n","epoch:212, avg_loss:0.010586692253127694\n","epoch:213, avg_loss:0.010967371752485633\n","epoch:214, avg_loss:0.010873870691284537\n","epoch:215, avg_loss:0.010394634446129203\n","epoch:216, avg_loss:0.010638108709827065\n","epoch:217, avg_loss:0.010595455532893538\n","epoch:218, avg_loss:0.010457183932885528\n","epoch:219, avg_loss:0.010366740636527538\n","epoch:220, avg_loss:0.010326630901545286\n","epoch:221, avg_loss:0.009869954315945506\n","epoch:222, avg_loss:0.009784633060917258\n","epoch:223, avg_loss:0.01002433686517179\n","epoch:224, avg_loss:0.009552473435178399\n","epoch:225, avg_loss:0.009474245132878423\n","epoch:226, avg_loss:0.0098080865573138\n","epoch:227, avg_loss:0.009722854476422071\n","epoch:228, avg_loss:0.009643995435908437\n","epoch:229, avg_loss:0.009587573818862438\n","epoch:230, avg_loss:0.009095116052776575\n","epoch:231, avg_loss:0.009418827947229147\n","epoch:232, avg_loss:0.009337903233245015\n","epoch:233, avg_loss:0.009183104848489165\n","epoch:234, avg_loss:0.00919012795202434\n","epoch:235, avg_loss:0.009124797186814249\n","epoch:236, avg_loss:0.008732808055356145\n","epoch:237, avg_loss:0.008662171778269112\n","epoch:238, avg_loss:0.008828802383504808\n","epoch:239, avg_loss:0.00883796950802207\n","epoch:240, avg_loss:0.008774857851676643\n","epoch:241, avg_loss:0.008536586887203157\n","epoch:242, avg_loss:0.008465640363283455\n","epoch:243, avg_loss:0.008395268698222935\n","epoch:244, avg_loss:0.008506727986969054\n","epoch:245, avg_loss:0.008090245304629207\n","epoch:246, avg_loss:0.008380427723750472\n","epoch:247, avg_loss:0.008026057970710099\n","epoch:248, avg_loss:0.008185859071090817\n","epoch:249, avg_loss:0.007907806080766022\n","epoch:250, avg_loss:0.0077971440041437745\n","epoch:251, avg_loss:0.007791600655764341\n","epoch:252, avg_loss:0.0074344787281006575\n","epoch:253, avg_loss:0.007625523838214576\n","epoch:254, avg_loss:0.00791152159217745\n","epoch:255, avg_loss:0.007849777000956237\n","epoch:256, avg_loss:0.007794456789270043\n","epoch:257, avg_loss:0.007753589656203985\n","epoch:258, avg_loss:0.007124118972569704\n","epoch:259, avg_loss:0.0076436618110165\n","epoch:260, avg_loss:0.007589037297293544\n","epoch:261, avg_loss:0.007251077680848539\n","epoch:262, avg_loss:0.007472729892469943\n","epoch:263, avg_loss:0.007148137199692428\n","epoch:264, avg_loss:0.007378199021331966\n","epoch:265, avg_loss:0.007326544378884137\n","epoch:266, avg_loss:0.006954721640795469\n","epoch:267, avg_loss:0.006906901020556688\n","epoch:268, avg_loss:0.006902001332491636\n","epoch:269, avg_loss:0.0071034817956388\n","epoch:270, avg_loss:0.006808288395404816\n","epoch:271, avg_loss:0.007019299082458019\n","epoch:272, avg_loss:0.0064672925509512424\n","epoch:273, avg_loss:0.006424521678127348\n","epoch:274, avg_loss:0.006824107840657234\n","epoch:275, avg_loss:0.006546791177242994\n","epoch:276, avg_loss:0.006794850574806333\n","epoch:277, avg_loss:0.006758380564861\n","epoch:278, avg_loss:0.006419143290258944\n","epoch:279, avg_loss:0.006508205784484744\n","epoch:280, avg_loss:0.006560309091582894\n","epoch:281, avg_loss:0.006582282483577728\n","epoch:282, avg_loss:0.006249206140637398\n","epoch:283, avg_loss:0.006495996844023466\n","epoch:284, avg_loss:0.006167472340166569\n","epoch:285, avg_loss:0.006164264748804271\n","epoch:286, avg_loss:0.006360988249070942\n","epoch:287, avg_loss:0.006319872918538749\n","epoch:288, avg_loss:0.006279526744037867\n","epoch:289, avg_loss:0.005970956874080002\n","epoch:290, avg_loss:0.005752663593739271\n","epoch:291, avg_loss:0.00611143431160599\n","epoch:292, avg_loss:0.005862161167897284\n","epoch:293, avg_loss:0.005822401028126478\n","epoch:294, avg_loss:0.005611221771687269\n","epoch:295, avg_loss:0.006018713116645813\n","epoch:296, avg_loss:0.005982146714814007\n","epoch:297, avg_loss:0.005945496843196452\n","epoch:298, avg_loss:0.005757323699072003\n","epoch:299, avg_loss:0.005609407438896596\n","####-------- inferece ---- -#####\n","0.9229499101638794\n"]}]}]}